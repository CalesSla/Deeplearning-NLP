{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import tempfile\n",
    "import logging\n",
    "import random\n",
    "import config\n",
    "import os\n",
    "import yaml\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from utilities import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from llama import BasicModelRunner\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "global_config = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"lamini_docs.jsonl\"\n",
    "dataset_path = f\"/content/{dataset_name}\"\n",
    "use_hf = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"lamini/lamini_docs\"\n",
    "use_hf = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"EleutherAI/pythia-70m\"\n",
    "\n",
    "training_config = {\n",
    "    \"model\": {\n",
    "        \"pretrained_name\": model_name,\n",
    "        \"max_length\": 2048\n",
    "    },\n",
    "    \"datasets\": {\n",
    "        \"use_hf\": use_hf,\n",
    "        \"path\": dataset_path\n",
    "    },\n",
    "    \"verbose\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 09:06:48,649 - DEBUG - utilities - Config: datasets.path: lamini/lamini_docs\n",
      "datasets.use_hf: true\n",
      "model.max_length: 2048\n",
      "model.pretrained_name: EleutherAI/pythia-70m\n",
      "verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize True lamini/lamini_docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 09:06:50,992 - DEBUG - fsspec.local - open file: C:/Users/User/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02/dataset_info.json\n",
      "2024-02-13 09:06:51,026 - DEBUG - fsspec.local - open file: C:/Users/User/.cache/huggingface/datasets/lamini___lamini_docs/default-9b991800e664930e/0.0.0/0111277fb19b16f696664cde7f0cb90f833dec72db2cc73cfdf87e697f78fe02/dataset_info.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1260\n",
      "})\n",
      "Dataset({\n",
      "    features: ['question', 'answer', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "train_dataset, test_dataset = tokenize_and_split_data(training_config, tokenizer)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-13 09:08:49,698 - DEBUG - __main__ - Select CPU device\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "device_count = torch.cuda.device_count()\n",
    "\n",
    "if device_count > 0:\n",
    "    logger.debug(\"Select GPU device\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    logger.debug(\"Select CPU device\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=100):\n",
    "    input_ids = tokenizer.encode(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_input_tokens\n",
    "    )\n",
    "\n",
    "    device=model.device\n",
    "    generated_tokens_with_prompt = model.generate(\n",
    "        input_ids=input_ids.to(device),\n",
    "        max_length=max_output_tokens\n",
    "    )\n",
    "\n",
    "    generated_text_with_prompt = tokenizer.batch_decode(generated_tokens_with_prompt, skip_special_tokens=True)\n",
    "    generated_text_answer = generated_text_with_prompt[0][len(text):]\n",
    "    return generated_text_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question input (test):  Can Lamini generate technical documentation or user manuals for software projects?\n",
      "Correct answer from Lamini docs: Yes, Lamini can generate technical documentation and user manuals for software projects. It uses natural language generation techniques to create clear and concise documentation that is easy to understand for both technical and non-technical users. This can save developers a significant amount of time and effort in creating documentation, allowing them to focus on other aspects of their projects.\n",
      "Model's answer: \n",
      "torch.Size([1, 15])\n",
      "no pkv\n",
      "torch.Size([1, 15])\n",
      "torch.Size([1, 16])\n",
      "torch.Size([1, 8, 15, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 17])\n",
      "torch.Size([1, 8, 16, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 18])\n",
      "torch.Size([1, 8, 17, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 19])\n",
      "torch.Size([1, 8, 18, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 20])\n",
      "torch.Size([1, 8, 19, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 21])\n",
      "torch.Size([1, 8, 20, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 22])\n",
      "torch.Size([1, 8, 21, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 23])\n",
      "torch.Size([1, 8, 22, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 24])\n",
      "torch.Size([1, 8, 23, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 25])\n",
      "torch.Size([1, 8, 24, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 26])\n",
      "torch.Size([1, 8, 25, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 27])\n",
      "torch.Size([1, 8, 26, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 28])\n",
      "torch.Size([1, 8, 27, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 29])\n",
      "torch.Size([1, 8, 28, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 8, 29, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 31])\n",
      "torch.Size([1, 8, 30, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 32])\n",
      "torch.Size([1, 8, 31, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 33])\n",
      "torch.Size([1, 8, 32, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 34])\n",
      "torch.Size([1, 8, 33, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 35])\n",
      "torch.Size([1, 8, 34, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 36])\n",
      "torch.Size([1, 8, 35, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 37])\n",
      "torch.Size([1, 8, 36, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 38])\n",
      "torch.Size([1, 8, 37, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 39])\n",
      "torch.Size([1, 8, 38, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 40])\n",
      "torch.Size([1, 8, 39, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 41])\n",
      "torch.Size([1, 8, 40, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 42])\n",
      "torch.Size([1, 8, 41, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 43])\n",
      "torch.Size([1, 8, 42, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 44])\n",
      "torch.Size([1, 8, 43, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 45])\n",
      "torch.Size([1, 8, 44, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 46])\n",
      "torch.Size([1, 8, 45, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 47])\n",
      "torch.Size([1, 8, 46, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 48])\n",
      "torch.Size([1, 8, 47, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 49])\n",
      "torch.Size([1, 8, 48, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 50])\n",
      "torch.Size([1, 8, 49, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 51])\n",
      "torch.Size([1, 8, 50, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 52])\n",
      "torch.Size([1, 8, 51, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 53])\n",
      "torch.Size([1, 8, 52, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 54])\n",
      "torch.Size([1, 8, 53, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 55])\n",
      "torch.Size([1, 8, 54, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 56])\n",
      "torch.Size([1, 8, 55, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 57])\n",
      "torch.Size([1, 8, 56, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 58])\n",
      "torch.Size([1, 8, 57, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 59])\n",
      "torch.Size([1, 8, 58, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 60])\n",
      "torch.Size([1, 8, 59, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 61])\n",
      "torch.Size([1, 8, 60, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 62])\n",
      "torch.Size([1, 8, 61, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 63])\n",
      "torch.Size([1, 8, 62, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 64])\n",
      "torch.Size([1, 8, 63, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 65])\n",
      "torch.Size([1, 8, 64, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 66])\n",
      "torch.Size([1, 8, 65, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 67])\n",
      "torch.Size([1, 8, 66, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 68])\n",
      "torch.Size([1, 8, 67, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 69])\n",
      "torch.Size([1, 8, 68, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 70])\n",
      "torch.Size([1, 8, 69, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 71])\n",
      "torch.Size([1, 8, 70, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 72])\n",
      "torch.Size([1, 8, 71, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 73])\n",
      "torch.Size([1, 8, 72, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 74])\n",
      "torch.Size([1, 8, 73, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 75])\n",
      "torch.Size([1, 8, 74, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 76])\n",
      "torch.Size([1, 8, 75, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 77])\n",
      "torch.Size([1, 8, 76, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 78])\n",
      "torch.Size([1, 8, 77, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 79])\n",
      "torch.Size([1, 8, 78, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 80])\n",
      "torch.Size([1, 8, 79, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 81])\n",
      "torch.Size([1, 8, 80, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 82])\n",
      "torch.Size([1, 8, 81, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 83])\n",
      "torch.Size([1, 8, 82, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 84])\n",
      "torch.Size([1, 8, 83, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 85])\n",
      "torch.Size([1, 8, 84, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 86])\n",
      "torch.Size([1, 8, 85, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 87])\n",
      "torch.Size([1, 8, 86, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 88])\n",
      "torch.Size([1, 8, 87, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 89])\n",
      "torch.Size([1, 8, 88, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 90])\n",
      "torch.Size([1, 8, 89, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 91])\n",
      "torch.Size([1, 8, 90, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 92])\n",
      "torch.Size([1, 8, 91, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 93])\n",
      "torch.Size([1, 8, 92, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 94])\n",
      "torch.Size([1, 8, 93, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 95])\n",
      "torch.Size([1, 8, 94, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 96])\n",
      "torch.Size([1, 8, 95, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 97])\n",
      "torch.Size([1, 8, 96, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 98])\n",
      "torch.Size([1, 8, 97, 64])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 99])\n",
      "torch.Size([1, 8, 98, 64])\n",
      "torch.Size([1, 1])\n",
      "\n",
      "\n",
      "I have a question about the following:\n",
      "\n",
      "How do I get the correct documentation to work?\n",
      "\n",
      "A:\n",
      "\n",
      "I think you need to use the following code:\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following code to get the correct documentation.\n",
      "\n",
      "A:\n",
      "\n",
      "You can use the following\n"
     ]
    }
   ],
   "source": [
    "test_text = test_dataset[0]['question']\n",
    "print(\"Question input (test): \", test_text)\n",
    "print(f\"Correct answer from Lamini docs: {test_dataset[0]['answer']}\")\n",
    "print(\"Model's answer: \")\n",
    "print(inference(test_text, base_model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3\n",
    "\n",
    "trained_model_name = f\"lamini_docs_{max_steps}_steps\"\n",
    "output_dir = trained_model_name\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    learning_rate = 1.0e-5,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=max_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=False,\n",
    "    disable_tqdm=False,\n",
    "    eval_steps=120,\n",
    "    save_steps=120,\n",
    "    warmup_steps=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=1,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(50304, 512)\n",
      "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
      "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
      "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
      ")\n",
      "Memory footprint 0.3084454 GB\n",
      "Flops 2195.667812352 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "model_flops = (\n",
    "  base_model.floating_point_ops(\n",
    "    {\n",
    "       \"input_ids\": torch.zeros(\n",
    "           (1, training_config[\"model\"][\"max_length\"])\n",
    "      )\n",
    "    }\n",
    "  )\n",
    "  * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "print(base_model)\n",
    "print(\"Memory footprint\", base_model.get_memory_footprint() / 1e9, \"GB\")\n",
    "print(\"Flops\", model_flops / 1e9, \"GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    model_flops=model_flops,\n",
    "    total_steps=max_steps,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c80dc32aa04e6f88c9aef4af533cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Trainer' object has no attribute 'do_grad_scaling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m training_output \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1506\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1504\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1511\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1801\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1800\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1801\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1804\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1805\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1806\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1807\u001b[0m ):\n\u001b[0;32m   1808\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1809\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\User\\Desktop\\Deeplearning NLP\\Finetuning LLMs\\utilities.py:276\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    274\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_grad_scaling\u001b[49m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'do_grad_scaling'"
     ]
    }
   ],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
